This file contains various notes and lessons learned concerning performance
of the Homa Linux kernel module.

* Problem: large messages have no pipelining. For example, copying bytes
  from user space to output buffers is not overlapped with sending packets,
  and copying bytes from buffers to user space doesn't start until the
  entire message has been received.
  * Tried overlapping packet transmission with packet creation (7/2019) but
    this made performance worse, not better. Not sure why.

* Waking up a process after a message arrives is expensive (a few microseconds).
  * Tried polling for messages (7/2019), but this made performance worse
    because the softirq handler runs on the same CPU as the receiving user
    process.

* It is hard for the pacer to keep the uplink fully utilized, because it
  gets descheduled for long periods of time.
  * Tried disabling interrupts while the pacer is running, but this doesn't
    work: if a packet gets sent with interrupts disabled, the interrupts get
    reenabled someplace along the way, which can lead to deadlock. Also,
    the VLAN driver uses "interrupts off" as a signal that it should polling
    mode, which doesn't work.
  * Tried calling homa_pacer_xmit from multiple places; this helps a bit
    (5-10%).
  * Tried making the pacer thread a high-priority real-time thread; this
    actually made things a bit worse.

* There can be a long lag in sending grants. One problem is that Linux
  tries to collect large numbers of buffers before invoking the softirq
  handler; this causes grants to be delayed. Implemented max_gro_skbs to
  limit buffering. However, varying the parameter doesn't seem to affect
  throughput (11/13/2019).

* Without RPS enabled, Homa performance is limited by a single core handling
  all softirq actions. In order for RPS to work well, Homa must implement
  its own hash function for mapping packets to cores (the default IP hasher
  doesn't know about Homa ports, so it considers only the peer IP address.
  However, with RPS, packets can get spread out over too many cores, which
  causes poor latency when there is a single client and the server is
  underloaded.

* Impact of load balancing on latency (xl170, 100B RPCs, 11/2019):
                    1 server thread  18 threads  TCP, 1 thread  TCP, 18 threads
No RPS/RFS             16.0 us         16.3 us      20.0 us        25.5 us
RPS/RFS enabled        17.1 us         21.5 us      21.9 us        26.5 us

* It's better to queue a waiting thread at the *front* of the list, rather
  than the rear (if there is a pool of server threads but not enough load
  to keep them all busy, it's better to reuse a few threads rather than
  spreading work across all of them; this produces better cash locality).
  This approach improves latency by 200-500ns at low loads.

